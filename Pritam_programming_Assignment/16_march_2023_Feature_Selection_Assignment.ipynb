{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5d3738",
   "metadata": {},
   "source": [
    "### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059cb1c",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8155e4",
   "metadata": {},
   "source": [
    "Overfitting :\n",
    "    \n",
    "    when overfit condition get occures , then machine Learning model has Low Bias and High Variance.\n",
    "    \n",
    "    Bias is training error.\n",
    "    \n",
    "    Variance is testing error.\n",
    "    \n",
    "    Low Bias is the low training error and High Variance is high testing error.\n",
    "    \n",
    "    In other words when model becomes overfit , \n",
    "    \n",
    "    it high training accuracy and low testing accuracy.\n",
    "    \n",
    "    \n",
    "    \n",
    "Underfitting:\n",
    "    \n",
    "    when underfit condition get occures , then machine Learning model has High Bias and High Variance.\n",
    "    \n",
    "    Bias is training error.\n",
    "    \n",
    "    Variance is testing error.\n",
    "    \n",
    "    High Bias is nothing but high training error and High Variance is nothing but high testing error.\n",
    "    \n",
    "    In other words when model becomes underfit ,\n",
    "    \n",
    "    It low training accuracy and low testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23bb09",
   "metadata": {},
   "source": [
    "Here we will discuss possible options to prevent overfitting, which helps improve the model performance.\n",
    "\n",
    "    1.Train with more data. \n",
    "    2.Data augmentation. \n",
    "    3.Addition of noise to the input data. \n",
    "    4.Feature selection. \n",
    "    5.Cross-validation. \n",
    "    6.Simplify data. \n",
    "    7.Regularization. \n",
    "    8.Ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde5104",
   "metadata": {},
   "source": [
    "Techniques to reduce underfitting: \n",
    "\n",
    "    1.Increase model complexity\n",
    "    2.Increase the number of features, performing feature engineering\n",
    "    3.Remove noise from the data.\n",
    "    4.Increase the number of epochs or increase the duration of training to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f5596",
   "metadata": {},
   "source": [
    "#### Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08654e6",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d3941",
   "metadata": {},
   "source": [
    "we can reduce overfitting :\n",
    "    \n",
    "    1.Train with more data. \n",
    "    2.Data augmentation. \n",
    "    3.Addition of noise to the input data. \n",
    "    4.Feature selection. \n",
    "    5.Cross-validation. \n",
    "    6.Simplify data. \n",
    "    7.Regularization. \n",
    "    8.Ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ceee9",
   "metadata": {},
   "source": [
    "Feature Selection :\n",
    "    \n",
    "    If we are use Feature selection method , then it helps reduce the overfitting.\n",
    "    \n",
    "    if we are not use feature selection method , so it can take all the features or those features.\n",
    "    \n",
    "    It is very clear that not all features contribute to the model. \n",
    "    \n",
    "    Some features are not important to the model and we can remove them.\n",
    "    \n",
    "    After creating a model, you can identify the most important features and remove the unnecessary features \n",
    "    \n",
    "    from the dataset and rebuild the model by using only the most important features. \n",
    "    \n",
    "    For Ensemble models like the random forest, XGBoost, this can be easily done using the feature_importances_ attribute. \n",
    "    \n",
    "    For linear regression and logistic regression models, backward elimination and forward selection can be done using \n",
    "    \n",
    "    separate classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616a643",
   "metadata": {},
   "source": [
    "#### Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eddb43",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164879f6",
   "metadata": {},
   "source": [
    "Underfitting:\n",
    "    \n",
    "    when underfit condition get occures , then machine Learning model has High Bias and High Variance.\n",
    "    \n",
    "    Bias is training error.\n",
    "\n",
    "    Variance is testing error.\n",
    "\n",
    "    High Bias is nothing but high training error and High Variance is nothing but high testing error.\n",
    "\n",
    "    In other words when model becomes underfit ,\n",
    "\n",
    "    It Low training accuracy and Low testing accuracy.\n",
    "    \n",
    "    \n",
    "Underfitting refers to a model that can neither performs well on the training data nor generalize to new data. \n",
    "\n",
    "Reasons of underfitting :\n",
    "    \n",
    "1.The size of the training dataset used is not enough.\n",
    "\n",
    "2.The model is too simple.\n",
    "\n",
    "3.Training data is not cleaned and also contains noise in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27433d17",
   "metadata": {},
   "source": [
    "#### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015769d",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad162d",
   "metadata": {},
   "source": [
    "when your model is low . that time bias is high and variance is high means it become underfit.\n",
    "\n",
    "when your model is high . that time bias is low and variance is high means it become overfit.\n",
    "\n",
    "we find middle way , where bias is low as well as variance is also low means model become most generalised model.\n",
    "\n",
    "that points is trade off point or bias - variance trade off.\n",
    "\n",
    "The relationship of bias and variance is :\n",
    "\n",
    "Bias is training error.\n",
    "\n",
    "Variance is testing error.\n",
    "\n",
    "it affect on accuracy of model. \n",
    "\n",
    "when model is overfiited so it is nice work on train data but poor on test data.\n",
    "\n",
    "when model is underfitted so it is poor work on train data as well poor work on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c192b32b",
   "metadata": {},
   "source": [
    "#### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702354a",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02d5de",
   "metadata": {},
   "source": [
    "We evaluate quantitatively overfitting / underfitting by using cross-validation. \n",
    "\n",
    "We calculate the mean squared error (MSE) on the validation set, the higher, \n",
    "\n",
    "the less likely the model generalizes correctly from the training data.\n",
    "\n",
    "we determine  model is overfitting or underfitting by this technique ,\n",
    "\n",
    "we check training accyracy and testing accuracy,\n",
    "\n",
    "if training accuracy is high and testing accuracy is low then model is overfitted.\n",
    "\n",
    "if training accuracy is low and testing accuracy is also low then model is underfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274b318",
   "metadata": {},
   "source": [
    "#### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886de0f4",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35562b3d",
   "metadata": {},
   "source": [
    "Overfitting :\n",
    "    \n",
    "    when overfit condition get occures , then machine Learning model has Low Bias and High Variance.\n",
    "\n",
    "    Bias is training error.\n",
    "\n",
    "    Variance is testing error.\n",
    "\n",
    "    Low Bias is the low training error and High Variance is high testing error.\n",
    "\n",
    "    In other words when model becomes overfit , \n",
    "\n",
    "    it high training accuracy and low testing accuracy.\n",
    "\n",
    "\n",
    "\n",
    "Underfitting:\n",
    "\n",
    "    when underfit condition get occures , then machine Learning model has High Bias and High Variance.\n",
    "\n",
    "    Bias is training error.\n",
    "\n",
    "    Variance is testing error.\n",
    "\n",
    "    High Bias is nothing but high training error and High Variance is nothing but high testing error.\n",
    "\n",
    "    In other words when model becomes underfit ,\n",
    "\n",
    "    It low training accuracy and low testing accuracy.\n",
    "    \n",
    "    \n",
    "\n",
    "some examples of high bias and high variance models :\n",
    "    \n",
    "    1.K Nearest neighbour (Knn)\n",
    "    \n",
    "    2.support vector machine (svm)\n",
    "    \n",
    "    3.CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62c978",
   "metadata": {},
   "source": [
    "#### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4428b77",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c454f",
   "metadata": {},
   "source": [
    "Overfitting impacts the accuracy of Machine Learning models. \n",
    "\n",
    "The model attempts to capture the data points that do not represent the accurate properties of data. \n",
    "\n",
    "These data points may be considered as noise. To avoid the occurrence of overfitting, \n",
    "\n",
    "we may use a method called regularization.\n",
    "\n",
    "Regularization is one of the most important concepts of machine learning. \n",
    "\n",
    "It is a technique to prevent the model from overfitting by adding extra information to it.\n",
    "\n",
    "Regularization works by adding a penalty or complexity term to the complex model. Let's consider the simple linear \n",
    "\n",
    "regression equation:\n",
    "\n",
    "y= β0+β1x1+β2x2+β3x3+⋯+βnxn +b\n",
    "\n",
    "In the above equation, Y represents the value to be predicted\n",
    "\n",
    "X1, X2, …Xn are the features for Y.\n",
    "\n",
    "β0,β1,…..βn are the weights or magnitude attached to the features, respectively. \n",
    "\n",
    "Here represents the bias of the model, and b represents the intercept.\n",
    "\n",
    "Linear regression models try to optimize the β0 and b to minimize the cost function. \n",
    "\n",
    "The equation for the cost function for the linear model is given below:\n",
    "\n",
    "Regularization in Machine Learning\n",
    "\n",
    "Now, we will add a loss function and optimize parameter to make the model that can predict the accurate value of Y.\n",
    "\n",
    "The loss function for the linear regression is called as RSS or Residual sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5075c",
   "metadata": {},
   "source": [
    "Techniques of Regularization\n",
    "\n",
    "There are mainly two types of regularization techniques, which are given below:\n",
    "\n",
    "    Ridge Regression\n",
    "    Lasso Regression\n",
    "\n",
    "Ridge Regression\n",
    "\n",
    "    Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can\n",
    "    \n",
    "    get better long-term predictions.Ridge regression is a regularization technique, which is used to reduce the complexity\n",
    "    \n",
    "    of the model. It is also called as L2 regularization. In this technique, the cost function is altered by adding the \n",
    "    \n",
    "    penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. \n",
    "    \n",
    "    We can calculate it by multiplying with the lambda to the squared weight of each individual feature. \n",
    "    \n",
    "    The equation for the cost function in ridge regression will be:\n",
    "\n",
    "Regularization in Machine Learning\n",
    "\n",
    "    In the above equation, the penalty term regularizes the coefficients of the model, and hence ridge regression reduces \n",
    "    \n",
    "    the amplitudes of the coefficients that decreases the complexity of the model. As we can see from the above equation,\n",
    "    \n",
    "    if the values of λ tend to zero, the equation becomes the cost function of the linear regression model. Hence, for the\n",
    "    \n",
    "    minimum value of λ, the model will resemble the linear regression model. A general linear or polynomial regression will\n",
    "    \n",
    "    fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can \n",
    "    \n",
    "    be used. It helps to solve the problems if we have more parameters than samples.\n",
    "\n",
    "Lasso Regression:\n",
    "\n",
    "    Lasso regression is another regularization technique to reduce the complexity of the model. \n",
    "    \n",
    "    It stands for Least Absolute and Selection Operator. It is similar to the Ridge Regression except that the penalty\n",
    "    \n",
    "    term contains only the absolute weights instead of a square of weights. Since it takes absolute values, hence, it can\n",
    "    \n",
    "    shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0. It is also called as L1 regularization.\n",
    "    \n",
    "    The equation for the cost function of Lasso regression will be:\n",
    "\n",
    "Regularization in Machine Learning\n",
    "\n",
    "    Some of the features in this technique are completely neglected for model evaluation.\n",
    "    \n",
    "    Hence, the Lasso regression can help us to reduce the overfitting in the model as well as the feature selection.\n",
    "\n",
    "Key Difference between Ridge Regression and Lasso Regression\n",
    "\n",
    "    Ridge regression is mostly used to reduce the overfitting in the model, and it includes all the features present in \n",
    "    \n",
    "    the model. It reduces the complexity of the model by shrinking the coefficients.\n",
    "    \n",
    "    Lasso regression helps to reduce the overfitting in the model as well as feature selection.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
